{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7837701-1158-4e7d-ba2e-cdba95ab54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 정규화 회귀 (Regularized Regression) 정의 및 목적\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 1. 정의:\n",
    "# - 일반 선형 회귀의 손실 함수(Loss Function)에 '페널티(Penalty)' 항을 추가하여 모델을 학습시키는 기법.\n",
    "# - 페널티 항은 가중치(회귀 계수, Beta)의 크기를 제한하는 역할을 함.\n",
    "\n",
    "# 2. 목적:\n",
    "# - 과적합 (Overfitting) 방지 및 모델의 일반화 성능 향상.\n",
    "# - 특성(Feature)이 너무 많거나 특성 간 다중 공선성(Multicollinearity)이 높을 때 모델의 안정성 확보.\n",
    "\n",
    "# 3. 핵심 원리:\n",
    "# - 정규화 목표 = min(잔차 제곱합 + 람다 * 페널티 항)\n",
    "# - 람다(λ): 규제 강도를 조절하는 하이퍼파라미터. 람다가 클수록 가중치가 0에 가깝게 줄어듦.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 주요 정규화 회귀 기법\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 1. Ridge 회귀 (L2 규제)\n",
    "# - 페널티 항: 가중치의 제곱합 (L2 Norm).  (∑ β_j^2)\n",
    "# - 특징: 모든 가중치를 0에 '가깝게' 줄이지만, 완전히 0으로 만들지는 않음.\n",
    "# - 주요 효과: 다중 공선성 문제 해결 및 모델 안정성 확보.\n",
    "\n",
    "# 2. Lasso 회귀 (L1 규제)\n",
    "# - 페널티 항: 가중치의 절댓값의 합 (L1 Norm). (∑ |β_j|)\n",
    "# - 특징: 불필요한 특성의 가중치를 '완전히 0'으로 만듦.\n",
    "# - 주요 효과: 특성 선택 (Feature Selection) 기능 제공, 모델을 간결하게 만듦.\n",
    "\n",
    "# 3. ElasticNet (L1 + L2 규제)\n",
    "# - 페널티 항: L1 규제와 L2 규제를 혼합하여 사용.\n",
    "# - 특징: Ridge와 Lasso의 장점을 결합하여 특성 선택과 모델 안정화를 동시에 추구.\n",
    "# - 활용: 상관관계가 높은 특성 그룹이 있을 때, Lasso의 단점(하나만 선택)을 보완하는 데 유용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d02f1c-bcab-4821-96f3-c644b17f07e8",
   "metadata": {},
   "source": [
    "# 가중치가 부여되는 예시 (Ridge 회귀 개념 포함)\n",
    "\n",
    "## 예시 상황\n",
    "학생들의 **공부시간(x1)**, **수면시간(x2)** 으로 **시험점수(y)** 를 예측한다고 하자.\n",
    "\n",
    "| 학생 | 공부시간(x1) | 수면시간(x2) | 시험점수(y) |\n",
    "|------|--------------|--------------|-------------|\n",
    "| A | 2 | 8 | 70 |\n",
    "| B | 4 | 7 | 80 |\n",
    "| C | 6 | 6 | 90 |\n",
    "| D | 8 | 5 | 95 |\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 모델 형태\n",
    "선형 회귀식:\n",
    "\n",
    "```\n",
    "y = w1*x1 + w2*x2 + b\n",
    "```\n",
    "\n",
    "- `w1`: 공부시간의 영향도  \n",
    "- `w2`: 수면시간의 영향도  \n",
    "- `b`: 기본 점수(절편)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 학습 전 (가중치 초기값은 랜덤)\n",
    "초기에는 모델이 아무것도 모른다. 예를 들어:\n",
    "\n",
    "```\n",
    "w1 = 0.5,  w2 = 0.5,  b = 50\n",
    "```\n",
    "\n",
    "| 학생 | 예측 y | 실제 y | 오차 |\n",
    "|------|---------|--------|------|\n",
    "| A | 0.5×2 + 0.5×8 + 50 = 54 | 70 | -16 |\n",
    "| B | 0.5×4 + 0.5×7 + 50 = 55.5 | 80 | -24.5 |\n",
    "| C | 0.5×6 + 0.5×6 + 50 = 56 | 90 | -34 |\n",
    "| D | 0.5×8 + 0.5×5 + 50 = 56.5 | 95 | -38.5 |\n",
    "\n",
    "오차가 크다. 모델이 패턴을 아직 모름.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 경사하강법으로 가중치 조정\n",
    "모델은 오차를 줄이기 위해 가중치를 조금씩 수정한다.\n",
    "\n",
    "```\n",
    "w_new = w_old - η * (∂Loss/∂w)\n",
    "```\n",
    "\n",
    "즉, 기울기 방향으로 조금씩 이동하면서 오차를 줄인다.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 학습 후 (가중치가 업데이트된 상태)\n",
    "여러 번 반복하면 모델이 아래와 같이 학습한다.\n",
    "\n",
    "```\n",
    "w1 = 5.0   # 공부시간의 영향이 큼  \n",
    "w2 = -2.0  # 수면시간이 많을수록 점수가 조금 줄어듦  \n",
    "b = 60\n",
    "```\n",
    "\n",
    "| 학생 | 예측 y | 실제 y | 오차 |\n",
    "|------|---------|--------|------|\n",
    "| A | 5×2 + (-2)×8 + 60 = 66 | 70 | -4 |\n",
    "| B | 5×4 + (-2)×7 + 60 = 74 | 80 | -6 |\n",
    "| C | 5×6 + (-2)×6 + 60 = 78 | 90 | -12 |\n",
    "| D | 5×8 + (-2)×5 + 60 = 90 | 95 | -5 |\n",
    "\n",
    "오차가 줄었고, 모델이 패턴을 이해함.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Ridge 회귀(L2 규제) 적용 시\n",
    "가중치가 너무 커지지 않도록 벌점을 추가한다.\n",
    "\n",
    "```\n",
    "Loss = (예측오차)^2 + α * (w1² + w2²)\n",
    "```\n",
    "\n",
    "가중치가 조금 더 안정적인 값으로 수렴함  \n",
    "예: `w1=4.5`, `w2=-1.8`, `b=60`\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 정리\n",
    "\n",
    "| 단계 | 설명 |\n",
    "|------|------|\n",
    "| 초기 | 모델은 가중치를 랜덤하게 정함 |\n",
    "| 학습 중 | 오차를 보고 가중치를 조금씩 조정 |\n",
    "| 학습 후 | 데이터의 패턴을 반영한 가중치로 수렴 |\n",
    "| Ridge 적용 시 | 큰 가중치에는 벌점 → 안정적인 값으로 유지 |\n",
    "\n",
    "---\n",
    "\n",
    "## 결론\n",
    "가중치는 모델이 입력값의 중요도를 스스로 학습한 결과이며,  \n",
    "Ridge 회귀는 그 가중치가 너무 커지지 않게 제어하는 방법이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49377d2a-d57b-48e9-a42c-49f481ee0643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
